{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = Path(os.getcwd())\n",
    "kaggle_dir = Path('kaggle/input/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = local_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = notebook_dir / 'llm-detect-ai-generated-text'\n",
    "external_data_dir = notebook_dir / 'external-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competition data\n",
    "train_essays = pd.read_csv(data_dir / 'train_essays.csv')\n",
    "train_essays['dataset'] = 'competition'\n",
    "train_prompts = pd.read_csv(data_dir / 'train_prompts.csv')\n",
    "test_essays = pd.read_csv(data_dir / 'test_essays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI generated data\n",
    "train_essays_ai = pd.read_csv(data_dir / 'train_essays_ai.csv')\n",
    "train_essays_ai['dataset'] = 'ai-generated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = train_essays.copy()\n",
    "training_data = pd.concat([training_data, train_essays_ai[['id', 'prompt_id', 'text', 'generated', 'dataset', 'model']]], axis=0)\n",
    "training_data.rename(columns={'model': 'source'}, inplace=True)\n",
    "training_data['source'].fillna('competition', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persuade data\n",
    "persuade_data = pd.read_csv(external_data_dir / 'Persuade' / 'persuade_2.csv')\n",
    "persuade_data = persuade_data[['prompt_name', 'text']]\n",
    "persuade_data.rename(columns={'prompt_name': 'prompt_id'}, inplace=True)\n",
    "persuade_data['generated'] = 0\n",
    "persuade_data['dataset'] = 'persuade'\n",
    "persuade_data['source'] = persuade_data['prompt_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# School Work data\n",
    "school_work_data = pd.read_csv(external_data_dir / 'SchoolWork' / 'school_work.csv')\n",
    "school_work_data.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H3 data\n",
    "h3_data = pd.read_json(external_data_dir / 'H3' / 'H3.jsonl', lines=True)\n",
    "h3_data = h3_data[['human_answers', 'chatgpt_answers', 'source']]\n",
    "h3_data.rename(columns={'human_answers': 'human', 'chatgpt_answers': 'ai_generated'}, inplace=True)\n",
    "h3_data['dataset'] = 'H3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M4 data\n",
    "m4_files = os.listdir(external_data_dir / 'M4')\n",
    "m4_data = []\n",
    "for file in m4_files:\n",
    "    if file.startswith('arxiv'):\n",
    "        file_data = pd.read_json(external_data_dir / 'M4' / file, lines=True)\n",
    "        file_data['file'] = file\n",
    "        file_data['source'] = 'arxiv'\n",
    "        m4_data.append(file_data)\n",
    "    elif file.startswith('wikipedia'):\n",
    "        file_data = pd.read_json(external_data_dir / 'M4' / file, lines=True)\n",
    "        file_data['file'] = file\n",
    "        file_data['source'] = 'wikipedia'\n",
    "        m4_data.append(file_data)\n",
    "        \n",
    "m4_data = pd.concat(m4_data)\n",
    "m4_data.rename(columns={'human_text': 'human', 'machine_text': 'ai_generated'}, inplace=True)\n",
    "\n",
    "m4_data = m4_data[['human', 'ai_generated', 'source']]\n",
    "m4_data.dropna(inplace=True)\n",
    "m4_data['dataset'] = 'M4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sherbold data\n",
    "sherbold_data = pd.read_csv(external_data_dir / 'sherbold-chatgpt-student-essay-study' / 'data' / 'essays-without-markers.csv', sep=';', encoding='UTF-8')\n",
    "sherbold_data.rename(columns={'Student': 'human', 'ChatGPT-4': 'ai_generated'}, inplace=True)\n",
    "sherbold_data = sherbold_data[['human', 'ai_generated']]\n",
    "sherbold_data['source'] = 'sherbold'\n",
    "sherbold_data['dataset'] = 'sherbold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(df):\n",
    "    new_df = df.copy()\n",
    "    for col in new_df.columns:\n",
    "        new_df[col] = new_df[col].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "        \n",
    "    return new_df\n",
    "\n",
    "def format_external_data(data):\n",
    "    human = data[['human', 'source', 'dataset']].copy()\n",
    "    human.rename(columns={'human': 'text'}, inplace=True)\n",
    "    human['generated'] = 0\n",
    "    \n",
    "    ai = data[['ai_generated', 'source', 'dataset']].copy()\n",
    "    ai.rename(columns={'ai_generated': 'text'}, inplace=True)\n",
    "    ai['generated'] = 1\n",
    "    \n",
    "    joined = pd.concat([human, ai])\n",
    "    joined['prompt_id'] = -1\n",
    "    joined = joined[['prompt_id', 'text', 'generated', 'dataset']]\n",
    "    joined = convert_list_to_string(joined)\n",
    "    \n",
    "    joined = joined[joined['text'] != '']\n",
    "    joined.dropna(inplace=True)\n",
    "    joined.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return convert_list_to_string(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.concat([training_data, persuade_data, school_work_data], axis=0)\n",
    "\n",
    "for dataset in [h3_data, m4_data, sherbold_data]:\n",
    "    training_data = pd.concat([training_data, format_external_data(dataset)], axis=0)\n",
    "training_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ARCHITECTURE = 'microsoft/deberta-v3-xsmall'\n",
    "INPUT_LENGTH = 1024\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ARCHITECTURE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ARCHITECTURE)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "persuade_train_prompts = ['Car-free cities', 'Driverless cars', 'The Face on Mars', 'Exploring Venus', 'Does the electoral college work?', 'Facial action coding system', '\"A Cowboy Who Rode the Waves\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define original data to be in train and validation sets\n",
    "original_training_data = training_data[\n",
    "    (training_data['dataset'] == 'competition') | \n",
    "    ((training_data['dataset'] == 'ai-generated') & (training_data['prompt_id'].isin(['0', '1'])))\n",
    "]\n",
    "\n",
    "original_validation_data = training_data[\n",
    "    (training_data['dataset'] == 'ai-generated') & \n",
    "    (training_data['prompt_id'].isin(persuade_train_prompts))\n",
    "]\n",
    "\n",
    "# Split persuade data based on prompts anticipated to be in competition test set\n",
    "persuade_data = training_data[training_data['dataset'] == 'persuade']\n",
    "persuade_train_prompts = ['Car-free cities', 'Driverless cars', 'The Face on Mars', 'Exploring Venus', 'Does the electoral college work?', 'Facial action coding system', '\"A Cowboy Who Rode the Waves\"']\n",
    "persuade_training_data = persuade_data[persuade_data['prompt_id'].isin(persuade_train_prompts)]\n",
    "persuade_validation_data = persuade_data[~persuade_data['prompt_id'].isin(persuade_train_prompts)]\n",
    "persuade_training_data = persuade_training_data.sample(frac=0.1, random_state=42)\n",
    "persuade_validation_data = persuade_validation_data.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Split out additional training data\n",
    "additional_training_data = training_data[\n",
    "    (training_data['dataset'] != 'competition') & \n",
    "    (training_data['dataset'] != 'ai-generated') &\n",
    "    (training_data['dataset'] != 'persuade')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split original training data into training and validation data\n",
    "model_training_data, model_validation_data = train_test_split(original_training_data, test_size=0.25, random_state=42)\n",
    "\n",
    "# Add persuade data to training and validation data\n",
    "model_training_data = pd.concat([model_training_data, persuade_training_data], axis=0)\n",
    "model_validation_data = pd.concat([model_validation_data, original_validation_data, persuade_validation_data], axis=0)\n",
    "model_training_data.reset_index(drop=True, inplace=True)\n",
    "model_validation_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/5 [                         ] 3.00% - Training Loss: 0.6923\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/claygendron/Git/Repos/llm-detect-ai-generated-text/baseline_model.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/claygendron/Git/Repos/llm-detect-ai-generated-text/baseline_model.ipynb#X31sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/claygendron/Git/Repos/llm-detect-ai-generated-text/baseline_model.ipynb#X31sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/claygendron/Git/Repos/llm-detect-ai-generated-text/baseline_model.ipynb#X31sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/claygendron/Git/Repos/llm-detect-ai-generated-text/baseline_model.ipynb#X31sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# Update batch count and calculate average loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/claygendron/Git/Repos/llm-detect-ai-generated-text/baseline_model.ipynb#X31sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m batch_num \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     adamw(\n\u001b[1;32m    185\u001b[0m         params_with_grad,\n\u001b[1;32m    186\u001b[0m         grads,\n\u001b[1;32m    187\u001b[0m         exp_avgs,\n\u001b[1;32m    188\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    189\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    190\u001b[0m         state_steps,\n\u001b[1;32m    191\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    192\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    193\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    194\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    195\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    196\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    197\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    198\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    199\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    200\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    201\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    202\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    203\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 335\u001b[0m func(\n\u001b[1;32m    336\u001b[0m     params,\n\u001b[1;32m    337\u001b[0m     grads,\n\u001b[1;32m    338\u001b[0m     exp_avgs,\n\u001b[1;32m    339\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    340\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    341\u001b[0m     state_steps,\n\u001b[1;32m    342\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    343\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    344\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    345\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    346\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    347\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    348\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    349\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    350\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    351\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    352\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[1;32m    353\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/optim/adamw.py:466\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m         denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 466\u001b[0m     param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n\u001b[1;32m    468\u001b[0m \u001b[39m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39mif\u001b[39;00m amsgrad \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Add Linear layer to model and send to device\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 2)\n",
    "model.to(device)\n",
    "\n",
    "# Create validation dataloader\n",
    "validation_torch = TorchDataset(model_validation_data['text'], model_validation_data['generated'], tokenizer, INPUT_LENGTH)\n",
    "validation_dataloader = DataLoader(validation_torch, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "for epoch in range(EPOCHS):  # Set a suitable number of epochs\n",
    "    model.train()\n",
    "    \n",
    "    # Add additional training data to training data\n",
    "    n_samples = int(len(model_training_data) / 2)\n",
    "    additional_training_data_sample = additional_training_data.sample(n=n_samples)\n",
    "    train_data = pd.concat([model_training_data, additional_training_data_sample], axis=0, ignore_index=True)\n",
    "    train_torch = TorchDataset(train_data['text'], train_data['generated'], tokenizer, INPUT_LENGTH)\n",
    "    train_dataloader = DataLoader(train_torch, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Progress bar\n",
    "    total_width = 25\n",
    "    print('Epoch {}/{} [{}] {:.2f}% - Training Loss: {}'.format(epoch, EPOCHS, ' '*total_width, 0, 0), end='\\r')\n",
    "    \n",
    "    # Training phase\n",
    "    total_loss = 0\n",
    "    batch_num = 0\n",
    "    batches_num = len(train_dataloader)\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update batch count and calculate average loss\n",
    "        batch_num += 1\n",
    "        avg_loss = total_loss / batch_num\n",
    "\n",
    "        # Progress bar\n",
    "        percent_completed = batch_num / batches_num\n",
    "        bars = int(percent_completed * total_width)\n",
    "        spaces = 25 - bars\n",
    "        print('Epoch {}/{} [{}{}] {:.2f}% - Training Loss: {:.4f}'.format(epoch, EPOCHS, '='*int(bars), ' '*int(spaces), percent_completed*100, avg_loss), end='\\r')\n",
    "\n",
    "\n",
    "    print('{}'.format(' '*int(500)), end='\\r') # clear progress bar\n",
    "    print(f'Epoch {epoch}/{EPOCHS} - Training Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    for batch in validation_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.nn.functional.softmax(logits, dim=-1)[:, 1]  # Get probabilities for the positive class\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Compute Log Loss and ROC AUC\n",
    "    log_loss_score = log_loss(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "    print(f'Validation Log Loss: {log_loss_score:.4f} - ROC AUC: {roc_auc:.4f}')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/claygendron/Git/Repos/llm-detect-ai-generated-text/baseline_model.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/claygendron/Git/Repos/llm-detect-ai-generated-text/baseline_model.ipynb#Y103sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/claygendron/Git/Repos/llm-detect-ai-generated-text/baseline_model.ipynb#Y103sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/claygendron/Git/Repos/llm-detect-ai-generated-text/baseline_model.ipynb#Y103sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/claygendron/Git/Repos/llm-detect-ai-generated-text/baseline_model.ipynb#Y103sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m logits \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/claygendron/Git/Repos/llm-detect-ai-generated-text/baseline_model.ipynb#Y103sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)[:, \u001b[39m1\u001b[39m]  \u001b[39m# Get probabilities for the positive class\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1304\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1304\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeberta(\n\u001b[1;32m   1305\u001b[0m     input_ids,\n\u001b[1;32m   1306\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1307\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1308\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1309\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1310\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1311\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1312\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1313\u001b[0m )\n\u001b[1;32m   1315\u001b[0m encoder_layer \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1316\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1066\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[39mif\u001b[39;00m token_type_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m-> 1066\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m   1067\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1068\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1069\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1070\u001b[0m     mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1071\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1072\u001b[0m )\n\u001b[1;32m   1074\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1075\u001b[0m     embedding_output,\n\u001b[1;32m   1076\u001b[0m     attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1080\u001b[0m )\n\u001b[1;32m   1081\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:878\u001b[0m, in \u001b[0;36mDebertaV2Embeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, mask, inputs_embeds)\u001b[0m\n\u001b[1;32m    875\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    877\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[1;32m    880\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids\u001b[39m.\u001b[39mlong())\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "# Validation phase\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "for batch in validation_dataloader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions = torch.nn.functional.softmax(logits, dim=-1)[:, 1]  # Get probabilities for the positive class\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "    all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Compute Log Loss and ROC AUC\n",
    "log_loss_score = log_loss(all_labels, all_predictions)\n",
    "roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "print(f'Validation ROC AUC: {roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.5985, -3.1306],\n",
       "        [-3.7707,  3.3330],\n",
       "        [-3.7994,  3.3930],\n",
       "        [-3.8172,  3.3911],\n",
       "        [ 2.2318, -1.9290],\n",
       "        [ 3.9097, -3.3366],\n",
       "        [-3.7991,  3.3547],\n",
       "        [-3.8044,  3.3574]], device='mps:0')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/deberta_baseline.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = torch.load('models/deberta_baseline.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/deberta_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, tokenizer, model, max_length):\n",
    "    # Ensure the model is in evaluation mode and on CPU\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Move tensors to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_cpu(text, tokenizer, model, max_length):\n",
    "    # Ensure the model is in evaluation mode and on CPU\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Move tensors to CPU\n",
    "    inputs = {k: v.cpu() for k, v in inputs.items()}\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I agree that changing to election by popular vote for the president of the United States would be a better choice than to continue Electoral College. You may ask yourself why. Well, it\\'s not fair to have to pick a random person to do something you should be the one doing. It\\'s like saying you pick your neighbor to clean your house. Your house is still being cleaned, but not by you, instead, a random person you don\\'t even know. Thus, continuing Electoral College is the same because in this case, the elector you pick and don\\'t even know is going to make choices that at the end of the day you might not even agree on and you can\\'t do nothing about it at that point.\\n\\nDuring electoral College voters get confused about the electors and vote for the wrong candidate. This means, there is too many names on the list that you most likely have never even heard in your life, therefore leading you to voting for the wrong candidate. Some electors aren\\'t even faithful, instead of going for their party\\'s candidate, they rather decide voting for whomever they please. Most likely leading to alot of conflict because the candidate is not even on their party and the voters would hate to have to follow someone else\\'s rules, yet again, can\\'t do nothing about it.\\n\\nVoters would always love to control whom their electors vote for, but this will never happen with Electoral College. The whole point to me for the Electoral College is so that the people voting for the electors won\\'t have any power against anything. So that the government continues to have power over the people. That can be great at some point, but sooner or later will cause confict because some people don\\'t always agree with what the government has to say or do. Therefore, changing to election by popular vote for president of U.S would be much better than to continue Electoral College.\\n\\nOn the article, \"The Indefensible Electoral College: Why even the bestlaid defenses of the system are wrong \". States that \"During the 2000 campaign, seventeen states didn\\'t see the candidates at all, including Rhode Island and South Carolina, and voters in 25 of the largest media markets didn\\'t get to see a single campaign ad.\" Now right there is a perfect example of electors being \"unfair\". Electors want people to keep voting for them so that they can pick the candidates, yet, they always skip the part of showing who the candidates are. There is no why people will randomly guess how their candidates will end up being if they never even hear a few words from them.\\n\\nIn conclusion, I strongly believe changing to election by popular vote for president of U.S is a wiser decision than to continue Electoral College because it\\'ll be fair to the people who are voting. People will start to get together to try and get to know the candidate, and most likely agree more to the idea than anything else. Mean while, Electoral College is a very unfair system because they have no idea who the elector is going to pick as candidate, and if they pick someone who they don\\'t even know at the end of the day they will be stuck with a total stranger.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_validation_data[model_validation_data['generated'] == 0]['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "      <th>dataset</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>82131f68</td>\n",
       "      <td>1</td>\n",
       "      <td>This essay will analyze, discuss and prove one...</td>\n",
       "      <td>1</td>\n",
       "      <td>competition</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>86fe4f18</td>\n",
       "      <td>1</td>\n",
       "      <td>I strongly believe that the Electoral College ...</td>\n",
       "      <td>1</td>\n",
       "      <td>competition</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>eafb8a56</td>\n",
       "      <td>0</td>\n",
       "      <td>Limiting car use causes pollution, increases c...</td>\n",
       "      <td>1</td>\n",
       "      <td>competition</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  prompt_id                                               text  \\\n",
       "704   82131f68          1  This essay will analyze, discuss and prove one...   \n",
       "740   86fe4f18          1  I strongly believe that the Electoral College ...   \n",
       "1262  eafb8a56          0  Limiting car use causes pollution, increases c...   \n",
       "\n",
       "      generated      dataset       source  \n",
       "704           1  competition  competition  \n",
       "740           1  competition  competition  \n",
       "1262          1  competition  competition  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_training_data[(original_training_data['dataset'] == 'competition') & (original_training_data['generated'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('models/test_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I agree that changing to election by popular vote for the president of the United States would be a better choice than to continue Electoral College. You may ask yourself why. Well, it\\'s not fair to have to pick a random person to do something you should be the one doing. It\\'s like saying you pick your neighbor to clean your house. Your house is still being cleaned, but not by you, instead, a random person you don\\'t even know. Thus, continuing Electoral College is the same because in this case, the elector you pick and don\\'t even know is going to make choices that at the end of the day you might not even agree on and you can\\'t do nothing about it at that point.\\n\\nDuring electoral College voters get confused about the electors and vote for the wrong candidate. This means, there is too many names on the list that you most likely have never even heard in your life, therefore leading you to voting for the wrong candidate. Some electors aren\\'t even faithful, instead of going for their party\\'s candidate, they rather decide voting for whomever they please. Most likely leading to alot of conflict because the candidate is not even on their party and the voters would hate to have to follow someone else\\'s rules, yet again, can\\'t do nothing about it.\\n\\nVoters would always love to control whom their electors vote for, but this will never happen with Electoral College. The whole point to me for the Electoral College is so that the people voting for the electors won\\'t have any power against anything. So that the government continues to have power over the people. That can be great at some point, but sooner or later will cause confict because some people don\\'t always agree with what the government has to say or do. Therefore, changing to election by popular vote for president of U.S would be much better than to continue Electoral College.\\n\\nOn the article, \"The Indefensible Electoral College: Why even the bestlaid defenses of the system are wrong \". States that \"During the 2000 campaign, seventeen states didn\\'t see the candidates at all, including Rhode Island and South Carolina, and voters in 25 of the largest media markets didn\\'t get to see a single campaign ad.\" Now right there is a perfect example of electors being \"unfair\". Electors want people to keep voting for them so that they can pick the candidates, yet, they always skip the part of showing who the candidates are. There is no why people will randomly guess how their candidates will end up being if they never even hear a few words from them.\\n\\nIn conclusion, I strongly believe changing to election by popular vote for president of U.S is a wiser decision than to continue Electoral College because it\\'ll be fair to the people who are voting. People will start to get together to try and get to know the candidate, and most likely agree more to the idea than anything else. Mean while, Electoral College is a very unfair system because they have no idea who the elector is going to pick as candidate, and if they pick someone who they don\\'t even know at the end of the day they will be stuck with a total stranger.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_validation_data[model_validation_data['source'] == 'competition']['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('models/deberta_baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models/deberta_tokenizer/tokenizer_config.json',\n",
       " 'models/deberta_tokenizer/special_tokens_map.json',\n",
       " 'models/deberta_tokenizer/spm.model',\n",
       " 'models/deberta_tokenizer/added_tokens.json',\n",
       " 'models/deberta_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save tokenizer\n",
    "tokenizer.save_pretrained('models/deberta_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Electoral College should be abolished because citizens can\\'t vote directly on which candidate they want. The electoral college is unfair because a candidate that wins the vote of the people, they can\\'t become president if they don\\'t have enough electoral votes. For example when Al Gore was elected, Gore won the popular vote but could not become president because of insufficient electoral college votes.\\n\\nIt is not fair to the people that their vote cannot count unless it goes through the Electoral College. The Electoral College bases on what the overall state vote is, instead of the individual voter, thus making it unfair to each voter if they do not get an equal say in who they want as president. Said in source two, \"Under the Electoral College system, voters vote not for the president, but for a slate of electors, who in turn elect the president.\" The citizens should be able to choose who governs their country. Direct elections are much easier than having an Electoral College. The voters vote, the majority wins and we have a new president.\\n\\nIn the Electoral College system, the voters vote of another set of voters who vote for the candidate of their party. The Electoral College way is much more difficult than having a popular vote method of voting.\\n\\nIn some cases,when no candidate wins a majority of the popular vote, in source 3, \"For example, Nixon in 1968 and clinton in 1992 both had only a 43 percent plurality of the popular votes, while winning a majority in the electoral college301 and 370 Electoral votes, respectively.\" This could be a clearer solution to having runoff elections. Also in source 3, \"There is no pressure for runoff elections when no candidate wins a majority of the voters cast that pressure, which would greatly complicate the presidentail election process, is reduced by the Electoral Collage, which invariably produces a clear winner.\" The president should be choosen by what the people want. There are also some citizens who may be a republican in a democratic state and vice versa who\\'s vote wouldn\\'t even be considered because the Electoral College will be voting for the overall states vote. In a popular vote everyone gets a say in who our president will be.\\n\\nHaving the popular vote method is a way to give all citizens a voice in who they want their next president to be. It\\'s also a way to get people involved with government. If everyone gets an equal say,Then the president is choosen fairly among the citizens instead of a group of electors that go by the states overall vote rather than everyone getting equality. After all, America is a democracy and the electoral college isn\\'t very democratic.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_validation_data[model_validation_data['source'] == 'competition']['text'].iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being generated text: 0.0034894279669970274 True value: 0\n",
      "Probability of being generated text: 0.018041841685771942 True value: 0\n",
      "Probability of being generated text: 0.005819542799144983 True value: 0\n",
      "Probability of being generated text: 0.09260311722755432 True value: 0\n",
      "Probability of being generated text: 0.005961323156952858 True value: 0\n",
      "Probability of being generated text: 0.0067536779679358006 True value: 0\n",
      "Probability of being generated text: 0.0049910759553313255 True value: 0\n",
      "Probability of being generated text: 0.032570913434028625 True value: 0\n",
      "Probability of being generated text: 0.003801137674599886 True value: 0\n",
      "Probability of being generated text: 0.12539352476596832 True value: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\n",
    "    text_to_predict = model_validation_data[model_validation_data['source'] == 'competition']['text'].iloc[i]\n",
    "    true_value = model_validation_data[model_validation_data['source'] == 'competition']['generated'].iloc[i]\n",
    "    \n",
    "    probabilities = predict_on_cpu(text_to_predict, tokenizer, model, INPUT_LENGTH)\n",
    "\n",
    "    # Assuming you're predicting binary class, get probability for the positive class\n",
    "    probability_positive_class = probabilities[0, 1].item()\n",
    "    print(\"Probability of being generated text:\", probability_positive_class, \"True value:\", true_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 384, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=384, out_features=2, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('models/deberta_baseline')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being generated text: 0.9987456798553467\n"
     ]
    }
   ],
   "source": [
    "text_to_predict = \"\"\"From an artist's perspective, streaming platforms provide unique opportunities. They offer a level playing field where both established and emerging artists can't reach global audiences without the traditional barriers of the music industry. This democratization has led to the discovery of new talents and diversified the music scene. While there are valid concerns regarding the revenue share and compensation for artists, the exposure and audience engagement these platforms provide are unparalleled. Streaming services also continually evolve their business models to address these concerns, indicating a positive trajectory for artist compensation.\"\"\"\n",
    "\n",
    "\n",
    "probabilities = predict(text_to_predict, tokenizer, model, INPUT_LENGTH)\n",
    "print(i, end='\\r')\n",
    "\n",
    "# Assuming you're predicting binary class, get probability for the positive class\n",
    "probability_positive_class = probabilities[0, 1].item()\n",
    "print(\"Probability of being generated text:\", probability_positive_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_work = pd.read_csv(notebook_dir / 'external-data' / 'SchoolWork' / 'school_work.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03435050696134567, 0.6802866458892822, 0.31053900718688965, 0.9831629991531372, 0.038362450897693634, 0.9600358605384827, 0.8826023936271667, 0.9009636044502258, 0.8143059015274048, 0.9965040683746338, 0.7934333086013794, 0.9547764658927917, 0.9610163569450378, 0.9479411244392395, 0.9045681953430176, 0.6042554974555969, 0.6808803081512451, 0.9443067312240601, 0.05708974972367287, 0.061988312751054764, 0.9008781313896179, 0.02312665991485119, 0.04241969808936119, 0.19519157707691193, 0.9974948167800903, 0.9882182478904724, 0.015881212428212166, 0.16377094388008118]\n",
      "models/deberta_baseline 0.6013696705922484\n",
      "[0.0016236385563388467, 0.07646387815475464, 0.06092625856399536, 0.9573656320571899, 0.0012614544248208404, 0.17942999303340912, 0.9507336020469666, 0.36756181716918945, 0.8159099817276001, 0.9998658895492554, 0.013205821625888348, 0.9808155298233032, 0.9998730421066284, 0.9977096319198608, 0.5718616247177124, 0.4564564526081085, 0.0038780204486101866, 0.04579993709921837, 0.04726201295852661, 0.015932312235236168, 0.6253079175949097, 0.0020113508217036724, 0.006631157360970974, 0.2635561525821686, 0.9996613264083862, 0.9979296922683716, 0.011131691746413708, 0.06331653892993927]\n",
      "models/test_model 0.411195798519267\n"
     ]
    }
   ],
   "source": [
    "for p in ['models/deberta_baseline', 'models/test_model']:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(p)\n",
    "    probs = []\n",
    "    for i, row in school_work.iterrows():\n",
    "        text_to_predict = row['text']\n",
    "        probabilities = predict_on_cpu(text_to_predict, tokenizer, model, INPUT_LENGTH)\n",
    "        school_work.loc[i, 'probability'] = probabilities[0, 1].item()\n",
    "        predict_proba = probabilities[0, 1].item()\n",
    "        probs.append(predict_proba)\n",
    "    print(probs)\n",
    "    print(p, np.mean(probs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
